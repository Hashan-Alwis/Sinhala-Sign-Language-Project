{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('DataSet')\n",
    "\n",
    "os.makedirs(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('DataSet')\n",
    "\n",
    "try:\n",
    "    os.makedirs(DATA_PATH)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Hashan\\\\Desktop\\\\Sign language'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "face_mesh = mp.solutions.face_mesh\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, face_mesh.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, face_mesh.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fdfs\n",
      "You entered: agsagaasdga\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "fdfs\n",
      "You entered: agsagaasdga\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "fdfs\n",
      "You entered: agsagaasdga\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "Stopped.\n"
     ]
    }
   ],
   "source": [
    "no_sequences=3\n",
    "sequence_length = 30\n",
    "\n",
    "\n",
    "while True:\n",
    "    action = input(\"Enter the meaning of action (type 'stop' to exit): \")\n",
    "    if action.lower() == \"stop\":\n",
    "        break\n",
    "\n",
    "    for sequence in range(0,no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "            print(\"fdfs\")\n",
    "            print(\"You entered:\", action)\n",
    "\n",
    "            cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "            with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                while cap.isOpened():\n",
    "\n",
    "                    # Read feed\n",
    "                    ret, frame = cap.read()\n",
    "\n",
    "                    # Make detections\n",
    "                    image, results = mediapipe_detection(frame, holistic)\n",
    "                    print(results)\n",
    "                    \n",
    "                    # Draw landmarks\n",
    "                    draw_styled_landmarks(image, results)\n",
    "\n",
    "                    for frame_num in range(sequence_length):\n",
    "                        if frame_num == 0: \n",
    "                            cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                            cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                            # Show to screen\n",
    "                            cv2.imshow('OpenCV Feed', image)\n",
    "                            cv2.waitKey(1000)\n",
    "                        else: \n",
    "                            cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                            # Show to screen\n",
    "                            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        \n",
    "\n",
    "                    # Break gracefully\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            pass\n",
    "\n",
    "\n",
    "print(\"Stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sequence number\n",
      "keypoints saved for sequence:  0\n",
      "1 sequence number\n",
      "keypoints saved for sequence:  1\n",
      "2 sequence number\n",
      "keypoints saved for sequence:  2\n",
      "0 sequence number\n",
      "keypoints saved for sequence:  0\n",
      "1 sequence number\n",
      "keypoints saved for sequence:  1\n",
      "2 sequence number\n",
      "keypoints saved for sequence:  2\n",
      "Stopped.\n"
     ]
    }
   ],
   "source": [
    "no_sequences=3\n",
    "sequence_length = 51\n",
    "\n",
    "\n",
    "while True:\n",
    "    action = input(\"Enter the meaning of action (type 'stop' to exit): \")\n",
    "    if action.lower() == \"stop\":\n",
    "        break\n",
    "    elif not action.strip():  \n",
    "        print(\"Input was empty. Please enter a valid action.\")\n",
    "        continue\n",
    "\n",
    "    # print(action)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "\n",
    "               \n",
    "            for sequence in range(0,no_sequences):\n",
    "                print(sequence , \"sequence number\")\n",
    "                try: \n",
    "                    os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "                    \n",
    "                    # print(\"folder created \", action, \" \", sequence )\n",
    "\n",
    "\n",
    "                    for frame_num in range(sequence_length):                \n",
    "                        ret, frame = cap.read()\n",
    "\n",
    "                        image, results = mediapipe_detection(frame, holistic)\n",
    "                        # print(results)\n",
    "                        \n",
    "                        draw_styled_landmarks(image, results)\n",
    "\n",
    "                        if frame_num < 5:\n",
    "                            # print(frame_num)\n",
    "                            cv2.putText(image, 'READY', (200, 200),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                        elif frame_num < 10:\n",
    "                            # print(frame_num)\n",
    "                            cv2.putText(image, '1', (300, 200),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                        elif frame_num < 15:\n",
    "                            # print(frame_num)\n",
    "                            cv2.putText(image, '2', (300, 200),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                        elif frame_num < 20:\n",
    "                            # print(frame_num)\n",
    "                            cv2.putText(image, '3', (300, 200),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                        elif frame_num < 25:\n",
    "                            # print(frame_num)\n",
    "                            cv2.putText(image, 'GO', (300, 200),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 4, cv2.LINE_AA)                    \n",
    "                        else:\n",
    "                            # print(frame_num) \n",
    "                            cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                            keypoints = extract_keypoints(results)\n",
    "                            # print(keypoints)\n",
    "                            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                            # print(npy_path)\n",
    "                            np.save(npy_path, keypoints)\n",
    "                    \n",
    "\n",
    "                            \n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                    \n",
    "\n",
    "                        # Break gracefully\n",
    "                        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                    print(\"keypoints saved for sequence: \", sequence)\n",
    "\n",
    "                        \n",
    "                    \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"An error occurred:\", e)\n",
    "                    pass\n",
    "\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "print(\"Stopped.\")                \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sequence number\n",
      "0 fra number\n",
      "1 fra number\n",
      "2 fra number\n",
      "3 fra number\n",
      "4 fra number\n",
      "1 sequence number\n",
      "0 fra number\n",
      "1 fra number\n",
      "2 fra number\n",
      "3 fra number\n",
      "4 fra number\n",
      "2 sequence number\n",
      "0 fra number\n",
      "1 fra number\n",
      "2 fra number\n",
      "3 fra number\n",
      "4 fra number\n"
     ]
    }
   ],
   "source": [
    "for sequence in range(0,3):\n",
    "    print(sequence , \"sequence number\")\n",
    "    for frame_num in range(5):\n",
    "        print(frame_num , \"fra number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "0\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "2\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "3\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "4\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "5\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "6\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "7\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "8\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "9\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "10\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "11\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "12\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "13\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "14\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "15\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "16\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "17\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "18\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "19\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "20\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "21\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "22\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "23\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "24\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "25\n",
      "[ 0.60092974  0.49842077 -0.51278877 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "26\n",
      "[ 0.60046184  0.49856913 -0.5030061  ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "27\n",
      "[ 0.59877312  0.49899295 -0.50790322 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "28\n",
      "[ 0.59872687  0.49917781 -0.59296709 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "29\n",
      "[ 0.59871262  0.49929014 -0.59685838 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "30\n",
      "[ 0.59834397  0.49923596 -0.57897174 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "31\n",
      "[ 0.59877121  0.4992303  -0.44990858 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "32\n",
      "[ 0.5982002   0.49925673 -0.46694216 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "33\n",
      "[ 0.59912223  0.4996253  -0.52365315 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "34\n",
      "[ 0.59911394  0.4996475  -0.57653296 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "35\n",
      "[ 0.59905797  0.49965733 -0.57452136 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "36\n",
      "[ 0.59949565  0.49939263 -0.59635419 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "37\n",
      "[ 0.59922266  0.4994812  -0.51855659 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "38\n",
      "[ 0.59923375  0.49950895 -0.5237276  ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "39\n",
      "[ 0.59911907  0.4996174  -0.5438875  ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "40\n",
      "[ 0.59897268  0.49940279 -0.54106319 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "41\n",
      "[ 0.59898841  0.49925566 -0.45929885 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "42\n",
      "[ 0.59900343  0.49918798 -0.47767502 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "43\n",
      "[ 0.59877121  0.49869213 -0.49257714 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "44\n",
      "[ 0.59877378  0.4986701  -0.50020468 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "45\n",
      "[ 0.59829181  0.49862951 -0.59276766 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "46\n",
      "[ 0.59754866  0.49850786 -0.47702274 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "47\n",
      "[ 0.59784293  0.49839923 -0.52305537 ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "48\n",
      "[ 0.59784657  0.49865866 -0.5530166  ...  0.          0.\n",
      "  0.        ]\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "49\n",
      "[ 0.59671909  0.49865615 -0.436508   ...  0.          0.\n",
      "  0.        ]\n",
      "Stopped.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Constants\n",
    "no_sequences = 3\n",
    "sequence_length =50\n",
    "\n",
    "# OpenCV\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    action = input(\"Enter the meaning of action (type 'stop' to exit): \")\n",
    "    if action.lower() == \"stop\":\n",
    "        break\n",
    "\n",
    "    print(action)\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            \n",
    "            for frame_num in range(sequence_length):\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                if frame_num < 5:\n",
    "                    print(frame_num)\n",
    "                    cv2.putText(image, 'READY', (200, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                elif frame_num < 10:\n",
    "                    print(frame_num)\n",
    "                    cv2.putText(image, '1', (300, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                elif frame_num < 15:\n",
    "                    print(frame_num)\n",
    "                    cv2.putText(image, '2', (300, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                elif frame_num < 20:\n",
    "                    print(frame_num)\n",
    "                    cv2.putText(image, '3', (300, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                elif frame_num < 25:\n",
    "                    print(frame_num)\n",
    "                    cv2.putText(image, 'GO', (300, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                else:\n",
    "                    print(frame_num) \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    print(keypoints)\n",
    "            \n",
    "\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Variables to track time and screens\n",
    "start_time = cv2.getTickCount()\n",
    "window_close_time = start_time + 3 * cv2.getTickFrequency()  # 3 seconds\n",
    "current_screen = 0\n",
    "\n",
    "# Loop to capture frames\n",
    "while True:\n",
    "    # Get the current time\n",
    "    current_time = cv2.getTickCount()\n",
    "    # Calculate elapsed time in seconds\n",
    "    elapsed_time = (current_time - start_time) / cv2.getTickFrequency()\n",
    "\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Create a blank image for the screens\n",
    "    screen = 255 * np.ones((frame.shape[0], frame.shape[1], 3), dtype=np.uint8)  # White image\n",
    "\n",
    "    # Display different screens sequentially\n",
    "    if elapsed_time < 1:\n",
    "        text = \"Screen 1\"\n",
    "    elif elapsed_time < 2:\n",
    "        text = \"Screen 2\"\n",
    "    else:\n",
    "        text = \"Screen 3\"\n",
    "    \n",
    "    # Add the text to the screen\n",
    "    cv2.putText(frame, text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    \n",
    "    # Combine the webcam frame and the screen\n",
    "    combined_frame = np.hstack((frame, screen))\n",
    "    \n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"Webcam Feed and Sequence\", combined_frame)\n",
    "    \n",
    "    # Check if it's time to close the window\n",
    "    # if current_time >= window_close_time:\n",
    "    #     break\n",
    "\n",
    "    # Wait for 1 millisecond\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign_language",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
